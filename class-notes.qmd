---
title: "MATH-472: Class Notes"
description: |
  Running qmd to keep notes during MATH-472 lectures.
author: "Andrew Moore"
knitr:
  opts_chunk: 
    echo: true
    warning: false
    message: false
---

```{r}
#| echo: false

library(tidyverse)
```

# 1/10/23

## Summary

Overview of syllabus. Covered introduction to R.

## Notes

- basic operators
  - `+ / - * ^ = <-`
- logical operators
- assignment (Dr. Ko prefers `<-`)
- `sum()`, `length()`, `c()`
- definition of functions
- if/then

# 1/12/23

## Summary

More preamble. Should've skipped, no new material, painful.

## Notes

Continuing where we left off. If/else statements.

- `ifelse()`
  - you can do assignment inside of this (wild, seems inadvisible, but neat)
  - e.g. `ifelse(1 > 2, x <- 1, x <- 0)`

```{r}
ex2 <- 76
ifelse(
  ex2 >= 90, grade <- 'A',
  ifelse(
    ex2 >= 80, grade <- 'B',
    ifelse(ex2 >= 70, grade <- 'C', '')
  )
)
```

- Arrays: used to store results of computations
  - one dimension: vectors e.g. `x <- numeric(k)` for a numeric vector of size `k`
  - two dimensions: matrices
    - $A_{2 \times 3}$
    - e.g. `y <- matrix(0, nrow = m, ncol = n)` for a `m` x `n` 0 matrix.
  - more dimensions
    - e.g. `l <- array(0, dim = c(m, n, ...))`

- covered indexing with matrices using bracket notation

### Loops

For loops

```{r}
k <- 0
for (i in 1:10) {
  k <- k + i
}

k

x <- 1:10
k <- 0

for (i in x) {
  k <- k + x[i]
}

k
```

While loops

```{r}
k <- 0
i <- 0

while (i <= 10) {
  k <- k + i
  i <- i + 1
}

k; i
```

### Practice, create a function that computes an average without using `sum()` or `mean()`

```{r}
avg <- function(x) {
  s <- 0
  for (i in x) s <- s + i

  s / i
}

x <- 1:10

avg(x) == 5.5
```

### `ifelse()` is vectorized, `if` is not

```{r}
score <- c(76, 92, 83)
grade <- ifelse(
  score >= 90, 'A',
  ifelse(score >= 80, 'B',
    ifelse(score >= 70, 'C')
  )
)
grade

# will fail-- length(score) > 1
if (score >= 90) {
  grade <- 'A'
} else if (score >= 80) {
  grade <- 'B'
} else {
  grade <- 'C'
}
```

### Note on statistical software

- R adapted from S Plus.

- Dr. Ko says this should be enough for us to cover the programming we'll use this semester.
  - recommends learning branching and loop syntax for adapting to other languages.

### Textbook chapter 2, Probability and Statistics Review

- **Random variable**: assigns outcomes in a sample space $\rightarrow$ Real numbers
  - describes behavior of population elements
  - **Probability Density Function** (pdf)
    - $f_X(x)$
  - **Cumulative Distribution Function** (cdf)
    - $F_X(x) \equiv P[X \leq x]$
    - non-decreasing function: $x_1 < x_2 \iff F_X(x_1) \leq F_X(x_2)$
    - right-continuous: $\lim_{\epsilon \to 0^{+}} F_X(x + \epsilon) = F_X(x), x \in \mathbb{R}$
    - $\lim_{x \to \infty} F_X(x) = 1, \lim_{x \to -\infty} F_X(x) = 0$.
    - CDFs for discrete functions are step functions.

# 1/17/23

# 1/19/23

# 1/24/23

## Summary

Discussing MLE, analytical solutions vs. numerical solutions.

## Notes

Review of analytical MLE is done in my notebook. Numerical estimation can be done using the `mle()` function, which is from the `stats4` package. This approach is only possible if we know the definition of the negative-loglik function. Also, depending on the starting value, it may take time for `mle()` to produce an estimate.

What kind of search is `mle()` using? It depends on `optim()`... which uses an implementation of *Nelder and Mead (1965).* 



```{r}
library(stats4)

# y <- c(0.04304550, 0.50263474)
y <- 1:4

mlogl <- function(theta) -(length(y)*log(theta) - theta*sum(y))

fit <- mle(mlogl, 1.4)
summary(fit)

1 / mean(y)
```

# 1/26/23

## Summary

## Notes

- data analysis: you have a sample
  - need to infer *population distribution function*
  - need to infer *parameters* of the distribution function
  - together, you use them to 

- MLE: used for parametric inference
  - MLE requires sample to be random/iid (likelihood function is a joint probability distribution). What if it's not?

- Today we're looking at random number generation?

### Two-parameter MLE

Suppose we have $x_1, x_2, ..., x_n \sim Gamma(\lambda, r)$, with pdf
$$
f(x; \lambda, r) = \begin{cases} \frac{\lambda}{\Gamma(r)} x^{r - 1} e^{-\lambda x}  & x \geq 0, r > 0, \lambda > 0 \\ 0, & \text{otherwise} \end{cases}$
$$

Note:
$$
\Gamma(r) = \int_0^\infty t^{r - 1} e^{-t}\ dt, t \geq 0.
$$

For a positive real number $r$, $\Gamma(r + 1) = r \cdot \Gamma(r).$

Note: $n! = \Gamma(n + 1)$ because $\Gamma(r + 1) = r \cdot \Gamma(r)$.

### the `optim()` function for MLE with >1 parameter

```{r}
x <- c(0.5928, 3.2048, 2.2281, 4.2292, 1.2096, 3.8371, 1.2670, 1.2042, 2.8182, 3.5173)

ll <- function(theta, sumx, sumlogx, n) {
  r <- theta[1]
  lambda <- theta[2]
  -(n*r*log(lambda) - n*log(gamma(r)) + (r - 1)*sumlogx - lambda*sumx)
}

optim(c(20, 19), ll, sumx = sum(x), sumlogx = sum(log(x)), n = length(x))
```

- even when estimating parameters numerically, you still need to be able to derive the log-likelihood function for the sample's distribution (or *assumed* distribution)

### Inverse Transform Method

1. continuous random variables

- $U$ is a uniform r.v. over $(a, b)$ if its pdf is $f_U(u) = \frac{1}{b-1}, a < u < b \text{ and } 0 \text{ otherwise}.$



2. discrete random variables

# 1/31/23

## Summary

## Notes

More on the Inverse Transform Method.

```{r}
# given an exponential distribution
lambda <- 10
exp_pdf <- function(x) lambda*exp(-lambda*x)
inv_exp <- function(u) -1/lambda * log(1 - u)

u <- runif(100000, 0, 1)
x <- inv_exp(u)

mean(x) # should be close to 1 / lambda
var(x) # should be close to 1 / lambda^2
```

Transformation of a discrete CDF via a quantile:

$y_i$ is the random variable corresponding to $u$ if
$$
F_Y(y_{i - 1}) < u \leq F_Y(y_i), u \sim Uniform(0, 1)
$$

```{r}
# transformation given a discrete CDF, using quantiles
g <- function(n) {
  u <- runif(n, 0, 1)
  
  cdf <- c(0.1, 0.36, 0.42, 0.85, 1)
  
  dplyr::case_when(
    u <= cdf[1]              ~ 0,
    u > cdf[1] & u <= cdf[2] ~ 1,
    u > cdf[2] & u <= cdf[3] ~ 2,
    u > cdf[3] & u <= cdf[4] ~ 3,
    u > cdf[4] & u <= cdf[5] ~ 4,
    TRUE ~ NA_real_
  )
}

prop.table(table(g(10000)))
cumsum(.Last.value)
```

# 2/2/2023

## Summary

## Notes

*Process of computational methods*

1. Estimate population distribution

2. Get point estimates of parameters

3. Simulate realizations from the population distribution
  - These realizations can then be used to test statistical methods/tools, etc.

```{r}
f <- function(x, p = 0.4) p * (1 - p)^x
```

What if we can't calculate a closed form of a CDF? Then, we can't do a tidy inverse transform...

### Acceptance-Rejection Method

We want realizations from a r.v. $X$, with pdf $f_X(x)$, but suppose it is hard to generate them. If we can find another pdf $g_X(x)$

1. which is defined on the same support of $f_X(x)$

2. from which it is easy to generate realizations,

and if we can find a constant $c > 1$ such that 

$$
h(x) = c \cdot g_X(x) \geq f_X(x)
$$

or

$$
\frac{f_X(x)}{g_X(x)} \leq c,
$$

then, we can generate a realization $x$ from $f_X(x)$ using the following algorithm:

1. Generate a realization $y$ from $g_X(x)$.
2. Generate a uniform random number $u$ from $U(0, 1)$.
3. Set $y$ to $x$ if $u < \frac{f_X(y)}{c\cdot g_X(y)}.$ Otherwise go back to step 1.

Then, $x$ is a random number with the pdf $f_X(x)$.

Probability that a point from step 1 is rejected is

$$
P(\text{A point generated from step 1 is rejected}) = \frac{\text{Area between } h(x) \text{and } \f(x)}{\text{Area under } h(x)}
$$

```{r}
# demonstrate acception-rejection method using a Beta(2, 1) distribution
# g(x) = Uniform(0, 1)

f <- function(x) 2*x
g <- function(x) 1
C <- 2

gen_beta <- function() {
  u1 <- runif(1, 0, 1)
  u2 <- runif(1, 0, 1)
  
  # if accepted,
  r <- f(u1) / (C * g(u1))
  out <- c( "r" = r, "u1" = u1, "u2" = u2)

  if (u2 < f(u1) / (C * g(u1))) {
    return(u1)
  } else {
    gen_beta()
  }
}

x <- replicate(1000, gen_beta())
x_actual <- rbeta(1000, 2, 1)

hist(x)
hist(x_actual)
```

# 2/7/2022

## Summary

## Notes

Acceptance-rejection method for discrete random variables.

Let $f_X(x) = \begin{cases} 0.16, x = 1 \\ 0.24, x = 2 \\ 0.33, x = 3 \\ 0.17, x = 4 \\ 0.10, x = 5 \\ 0, \text{ otherwise}. \end{cases}$ Assume we also have $g_X(x) = \frac{1}{5}, x \in \{1, 2, 3, 4, 5\} \text{ and } 0 \text{ otherwise}.$

```{r}
f <- c(0.16, 0.24, 0.33, 0.17, 0.10)
g <- c(1, 1, 1, 1, 1) / 5
C <- max(f / g)

f_x <- function(x) {
  dplyr::case_when(
    x == 1 ~ 0.16,
    x == 2 ~ 0.24,
    x == 3 ~ 0.33,
    x == 4 ~ 0.17,
    x == 5 ~ 0.10,
      TRUE ~ 0
  )
}

gen <- function() {
  y <- sample(1:5, 1) # draw 1 realization from discrete uniform
  u <- runif(1, 0, 1) # draw 1 realization from continuous uniform

  if (u <= f_x(y) / (C * 1/5)) {
    return(y)
  } else {
    gen()
  }
}

out <- replicate(1000, gen())
prop.table(table(out))
```