---
title: "MATH-472: Class Notes"
description: |
  My running notes during MATH-472 lectures.
author: "Andrew Moore"
format:
  html:
    toc-depth: 1
knitr:
  opts_chunk: 
    echo: true
    warning: false
    message: false
---

```{r}
#| echo: false

library(tidyverse)
set.seed(8686)

theme_set(
  theme_minimal(base_size = 15) +
    theme(plot.background = element_rect(fill = "white"))
)
```

# 1/10/23

## Summary

Overview of syllabus. Covered introduction to R.

## Notes

- basic operators
  - `+ / - * ^ = <-`
- logical operators
- assignment (Dr. Ko prefers `<-`)
- `sum()`, `length()`, `c()`
- definition of functions
- if/then

# 1/12/23

## Summary

More preamble. Should've skipped, no new material, painful.

## Notes

Continuing where we left off. If/else statements.

- `ifelse()`
  - you can do assignment inside of this (wild, seems inadvisible, but neat)
  - e.g. `ifelse(1 > 2, x <- 1, x <- 0)`

```{r}
ex2 <- 76
ifelse(
  ex2 >= 90, grade <- 'A',
  ifelse(
    ex2 >= 80, grade <- 'B',
    ifelse(ex2 >= 70, grade <- 'C', '')
  )
)
```

- Arrays: used to store results of computations
  - one dimension: vectors e.g. `x <- numeric(k)` for a numeric vector of size `k`
  - two dimensions: matrices
    - $A_{2 \times 3}$
    - e.g. `y <- matrix(0, nrow = m, ncol = n)` for a `m` x `n` 0 matrix.
  - more dimensions
    - e.g. `l <- array(0, dim = c(m, n, ...))`

- covered indexing with matrices using bracket notation

### Loops

For loops

```{r}
k <- 0
for (i in 1:10) {
  k <- k + i
}

k

x <- 1:10
k <- 0

for (i in x) {
  k <- k + x[i]
}

k
```

While loops

```{r}
k <- 0
i <- 0

while (i <= 10) {
  k <- k + i
  i <- i + 1
}

k; i
```

### Practice, create a function that computes an average without using `sum()` or `mean()`

```{r}
avg <- function(x) {
  s <- 0
  for (i in x) s <- s + i

  s / i
}

x <- 1:10

avg(x) == 5.5
```

### `ifelse()` is vectorized, `if` is not

```{r}
score <- c(76, 92, 83)
grade <- ifelse(
  score >= 90, 'A',
  ifelse(score >= 80, 'B',
    ifelse(score >= 70, 'C')
  )
)
grade

# will fail-- length(score) > 1
# if (score >= 90) {
#   grade <- 'A'
# } else if (score >= 80) {
#   grade <- 'B'
# } else {
#   grade <- 'C'
# }
```

### Note on statistical software

- R adapted from S Plus.

- Dr. Ko says this should be enough for us to cover the programming we'll use this semester.
  - recommends learning branching and loop syntax for adapting to other languages.

### Textbook chapter 2, Probability and Statistics Review

- **Random variable**: assigns outcomes in a sample space $\rightarrow$ Real numbers
  - describes behavior of population elements
  - **Probability Density Function** (pdf)
    - $f_X(x)$
  - **Cumulative Distribution Function** (cdf)
    - $F_X(x) \equiv P[X \leq x]$
    - non-decreasing function: $x_1 < x_2 \iff F_X(x_1) \leq F_X(x_2)$
    - right-continuous: $\lim_{\epsilon \to 0^{+}} F_X(x + \epsilon) = F_X(x), x \in \mathbb{R}$
    - $\lim_{x \to \infty} F_X(x) = 1, \lim_{x \to -\infty} F_X(x) = 0$.
    - CDFs for discrete functions are step functions.

# 1/17/23

- Absent

# 1/19/23

- Absent

# 1/24/23

## Summary

Discussing MLE, analytical solutions vs. numerical solutions.

## Notes

Review of analytical MLE is done in my notebook. Numerical estimation can be done using the `mle()` function, which is from the `stats4` package. This approach is only possible if we know the definition of the negative-loglik function. Also, depending on the starting value, it may take time for `mle()` to produce an estimate.

What kind of search is `mle()` using? It depends on `optim()`... which uses an implementation of *Nelder and Mead (1965).* 



```{r}
library(stats4)

# y <- c(0.04304550, 0.50263474)
y <- 1:4

mlogl <- function(theta) -(length(y)*log(theta) - theta*sum(y))

fit <- mle(mlogl, 1.4)
summary(fit)

1 / mean(y)
```

# 1/26/23

## Summary

Two parameter MLE, `optim()`, Inverse Transform Method

## Notes

- data analysis: you have a sample
  - need to infer *population distribution function*
  - need to infer *parameters* of the distribution function
  - together, you use them to 

- MLE: used for parametric inference
  - MLE requires sample to be random/iid (likelihood function is a joint probability distribution). What if it's not?

- Today we're looking at random number generation?

### Two-parameter MLE

Suppose we have $x_1, x_2, ..., x_n \sim Gamma(\lambda, r)$, with pdf
$$
f(x; \lambda, r) = \begin{cases} \frac{\lambda}{\Gamma(r)} x^{r - 1} e^{-\lambda x}  & x \geq 0, r > 0, \lambda > 0 \\ 0, & \text{otherwise} \end{cases}$
$$

Note:
$$
\Gamma(r) = \int_0^\infty t^{r - 1} e^{-t}\ dt, t \geq 0.
$$

For a positive real number $r$, $\Gamma(r + 1) = r \cdot \Gamma(r).$

Note: $n! = \Gamma(n + 1)$ because $\Gamma(r + 1) = r \cdot \Gamma(r)$.

### the `optim()` function for MLE with >1 parameter

```{r}
x <- c(0.5928, 3.2048, 2.2281, 4.2292, 1.2096, 3.8371, 1.2670, 1.2042, 2.8182, 3.5173)

ll <- function(theta, sumx, sumlogx, n) {
  r <- theta[1]
  lambda <- theta[2]
  -(n*r*log(lambda) - n*log(gamma(r)) + (r - 1)*sumlogx - lambda*sumx)
}

optim(c(20, 19), ll, sumx = sum(x), sumlogx = sum(log(x)), n = length(x))
```

- even when estimating parameters numerically, you still need to be able to derive the log-likelihood function for the sample's distribution (or *assumed* distribution)

### Inverse Transform Method

1. continuous random variables

- $U$ is a uniform r.v. over $(a, b)$ if its pdf is $f_U(u) = \frac{1}{b-1}, a < u < b \text{ and } 0 \text{ otherwise}.$



2. discrete random variables

# 1/31/23

## Summary

More on the Inverse Transform Method.

## Notes

```{r}
# given an exponential distribution
lambda <- 10
exp_pdf <- function(x) lambda*exp(-lambda*x)
inv_exp <- function(u) -1/lambda * log(1 - u)

u <- runif(100000, 0, 1)
x <- inv_exp(u)

mean(x) # should be close to 1 / lambda
var(x) # should be close to 1 / lambda^2
```

Transformation of a discrete CDF via a quantile:

$y_i$ is the random variable corresponding to $u$ if
$$
F_Y(y_{i - 1}) < u \leq F_Y(y_i), u \sim Uniform(0, 1)
$$

```{r}
# transformation given a discrete CDF, using quantiles
g <- function(n) {
  u <- runif(n, 0, 1)
  
  cdf <- c(0.1, 0.36, 0.42, 0.85, 1)
  
  dplyr::case_when(
    u <= cdf[1]              ~ 0,
    u > cdf[1] & u <= cdf[2] ~ 1,
    u > cdf[2] & u <= cdf[3] ~ 2,
    u > cdf[3] & u <= cdf[4] ~ 3,
    u > cdf[4] & u <= cdf[5] ~ 4,
    TRUE ~ NA_real_
  )
}

p <- prop.table(table(g(10000)))
cumsum(p)
```

# 2/2/2023

## Summary

Process of computational methods, acceptance-rejection method

## Notes

*Process of computational methods*

1. Estimate population distribution

2. Get point estimates of parameters

3. Simulate realizations from the population distribution
  - These realizations can then be used to test statistical methods/tools, etc.

```{r}
f <- function(x, p = 0.4) p * (1 - p)^x
```

What if we can't calculate a closed form of a CDF? Then, we can't do a tidy inverse transform...

### Acceptance-Rejection Method

We want realizations from a r.v. $X$, with pdf $f_X(x)$, but suppose it is hard to generate them. If we can find another pdf $g_X(x)$

1. which is defined on the same support of $f_X(x)$

2. from which it is easy to generate realizations,

and if we can find a constant $c > 1$ such that 

$$
h(x) = c \cdot g_X(x) \geq f_X(x)
$$

or

$$
\frac{f_X(x)}{g_X(x)} \leq c,
$$

then, we can generate a realization $x$ from $f_X(x)$ using the following algorithm:

1. Generate a realization $y$ from $g_X(x)$.
2. Generate a uniform random number $u$ from $U(0, 1)$.
3. Set $y$ to $x$ if $u < \frac{f_X(y)}{c\cdot g_X(y)}.$ Otherwise go back to step 1.

Then, $x$ is a random number with the pdf $f_X(x)$.

Probability that a point from step 1 is rejected is

$$
P(\text{A point generated from step 1 is rejected}) = \frac{\text{Area between } h(x) \text{and } f(x)}{\text{Area under } h(x)}
$$

```{r}
# demonstrate acception-rejection method using a Beta(2, 1) distribution
# g(x) = Uniform(0, 1)

f <- function(x) 2*x
g <- function(x) 1
C <- 2

gen_beta <- function() {
  u1 <- runif(1, 0, 1)
  u2 <- runif(1, 0, 1)
  
  # if accepted,
  r <- f(u1) / (C * g(u1))
  out <- c( "r" = r, "u1" = u1, "u2" = u2)

  if (u2 < f(u1) / (C * g(u1))) {
    return(u1)
  } else {
    gen_beta()
  }
}

x <- replicate(1000, gen_beta())
x_actual <- rbeta(1000, 2, 1)

hist(x)
hist(x_actual)
```

# 2/7/2022

## Summary

Acceptance-rejection method for discrete random variables

## Notes

Acceptance-rejection method for discrete random variables.

Let $f_X(x) = \begin{cases} 0.16, x = 1 \\ 0.24, x = 2 \\ 0.33, x = 3 \\ 0.17, x = 4 \\ 0.10, x = 5 \\ 0, \text{ otherwise}. \end{cases}$ Assume we also have $g_X(x) = \frac{1}{5}, x \in \{1, 2, 3, 4, 5\} \text{ and } 0 \text{ otherwise}.$

```{r}
f <- c(0.16, 0.24, 0.33, 0.17, 0.10)
g <- c(1, 1, 1, 1, 1) / 5
C <- max(f / g)

f_x <- function(x) {
  dplyr::case_when(
    x == 1 ~ 0.16,
    x == 2 ~ 0.24,
    x == 3 ~ 0.33,
    x == 4 ~ 0.17,
    x == 5 ~ 0.10,
      TRUE ~ 0
  )
}

gen <- function() {
  y <- sample(1:5, 1) # draw 1 realization from discrete uniform
  u <- runif(1, 0, 1) # draw 1 realization from continuous uniform

  if (u <= f_x(y) / (C * 1/5)) {
    return(y)
  } else {
    gen()
  }
}

out <- replicate(1000, gen())
prop.table(table(out))
```

# 2/9/2023

## Summary

Covering random processes (counting processes, poisson processes, renewal processes).

## Notes

- random processes depend on an **index**, often *time*

```{r}
gen_poisson <- function(t0, lambda) {
  s <- 0; i <- 0

  while (s <= t0) {
    u <- runif(1, 0, 1)
    t <- -log(1 - u) / lambda  # inverse transform to draw from Exp(lambda)
    s <- s + t
    i <- i + 1
  }

  return(i - 1)
}

gen_poisson(10, 2)

x <- replicate(1000, gen_poisson(8, 3))

hist(x)
```

# 2/21/23

## Summary

Monte-Carlo Integration, `rnorm()`, `qnorm()`, etc.

## Notes

- We'll get back the exam results this Thursday.
  - We'll have a chance to correct scores (if they're really bad?)

- We're skipping chapter 5 (which covers data visualization), and move to chapter 6

- Chapter 6 utilizes the process of generating random numbers (ch. 3), but we won't use methods such as inverse-transform; we'll use built-in methods from R

$x_1, x_2, \cdots, x_n \sim Exp(\lambda)$

$$
f_X(x) = \begin{cases}
  \lambda \cdot e^{-\lambda x} & x \geq 0, \lambda > 0 \\
  0 & \text{ otherwise.}
\end{cases}
$$

$u \equiv F_X(x) = 1 - e^{-\lambda x}$
$x \to -\frac{1}{\lambda}ln(1 - u)$

```{r}
lambda <- 3
n <- 10
x <- 1
p <- 0.7

rexp(n, lambda) # x_i ~ f(x)             -- random draws
pexp(x, lambda) # P(X <= x)              -- CDF, cumulative probability
dexp(x, lambda) # P(X == x)?             -- density
qexp(p, lambda) # x* s.t. P(X <= x*) = p -- quantile
```

```{r}
mu <- 5
sd <- 3

rnorm(1, mu, sd)
pnorm(5, mu, sd)
qnorm(0.75, mu, sd)
```

- covering use of `set.seed()`

Example. Suppose $u \sim U(0, 1)$, and
$$
F_u(u^*) \equiv P(U \leq u^*) = \int_0^{u^*} f_u(u)\ du = \int_0^{u^*} 1\ du = u^*
$$

Demonstration of theorem, using $punif()$.

```{r}
punif(0.6, 0, 1)
```

### Monte Carlo Integration

<!-- hell yea, new topics -->

$$
\int_0^1 e^{-x}\ dx
$$

In numerical integration (Simpson's rule, midpoint rule, etc.), you can learn to integrate something like the above.

However, in statistics, we accomplish this by generating random numbers.

```{r}
u <- runif(1000000, 0, 1)
mean(exp(-u))

1 - exp(-1)
```

Monte Carlo integration relies on the *Strong Law of Large Numbers.*

Let $g(x)$ be a function of random variable $X$ with pdf $f_X(x)$. Then let
$$
\theta = E(g(X)) = \int_{-\infty}^{\infty} g(x)f_X(x)\ dx
$$

If one has a random sample of size $n$, $x_1, x_2, \cdots, x_n$ from $f_X(x)$, then $\hat{\theta} = \sum_{i = 1}^{n} g(x_i) / n$ converges to $\theta$ with probability 1 by the Strong Law of Large Numbers.

Example:

$$
\theta = E(g(X)) = \int_0^1 x\ dx = \int_0^1 x \cdot \frac{1}{1 - 0}\ dx = \frac{1}{2}
$$

```{r}
n1 <- 500
n2 <- 5000

u1 <- runif(n1, 0, 1)
u2 <- runif(n2, 0, 1)

# g(x) = x
mean(u1)
mean(u2)
```

$$
\theta = \int_a^b g(x)\ dx = (b - a) \cdot \int_a^b g(x) \cdot \frac{1}{b - a}\ dx
$$

```{r}
-exp(-5) - (-exp(-2))

u <- runif(1000, 2, 5)
(5 - 2) * mean(exp(-u))
```

# 2/23/23

## Summary

More on Monte-Carlo integration, discussion of why sample mean is preferred vs. other estimators wrt the law of large numbers.

## Notes

- turn in corrections to your exam to Dr. Ko's office, by 12pm tomorrow.

- Why do we like the sample mean (for estimating $\mu$)?
  - it uses the same formula! this is why the strong law of large numbers is powerful
  - sample median won't necessarily converge to population mean, e.g. when the population distribution isn't symmetric

```{r}
library(purrr)

mc_integrate <- function(g, a = 0, b = 1, n) {
  u <- runif(n, a, b)
  (b - a) * mean(g(u))
}

f <- function(x) exp(-x)

map_dbl(c(10, 100, 1000, 10000, 100000), ~mc_integrate(f, 2, 4, .))

# true value
exp(-2) - exp(-4)

h <- function(x) 10 * exp(-10 * x)
map_dbl(c(10, 1000, 1000000), ~mc_integrate(h, 0, 2, .))

# true value
integrate(h, 0, 2)
```

Currently, we've been checking the difference between our estimates and the true value. This is a measure of *bias* against true parameter values. However, we'll discuss measurements of variance in a future class.

# 2/28/23

## Summary

## Notes

- Homework 3 assigned
- Exam corrections returned (I got 89%?)

###

$Pr(0 \leq x \leq 5)$ where
$$
f_X(x) = \begin{cases}
10 \cdot e^{-10x}, & x \geq 0 \\
0, & \text{ otherwise}
\end{cases}
$$

```{r}
f <- function(x) 10 * exp(-10 * x)
mc_integrate(f, 0, 5, 100000)
```

```{r}
cdf_exp_b <- function(x, lambda, n) {
  u <- runif(n, 0, 1)
  x * mean(lambda * exp(-lambda * x * u))
}

u <- cdf_exp_b(6, 20, 100)
l <- cdf_exp_b(2, 20, 100)

u - l

pexp(6, 1/20) - pexp(2, 1/20)
```

### Example from class: write a function to generate a Z-table

```{r}
# Compare results to textbook example 6.3



z_cdf <- function(x, n = 1000) {
  u <- runif(n, 0, 1)

  # density of Z ~ N(0, 1)?
  theta <- function(x, u) {
    mean(1 / sqrt(2 * pi) * x * exp((-x^2 * u^2) / 2))
  }

  # Z ~ N(0, 1) is symmetric about 0
  if (x >= 0) {
    0.5 + theta(x, u)
  } else {
    1 - 0.5 - theta(-x, u)
  }
}

z_cdf(2, 100000)
pnorm(2)
```


# 3/2/23

## Summary

- "Hit or Miss" approach to Monte Carlo Integration


## Notes

Today: An easier way to generate CDFs for random variables?

"Hit or Miss" approach for Monte Carlo Integration.

Let $f(x)$ be the pdf of a random variable $X$. To estimate $F(x) = \int_{-\infty}^x f(t)\ dt$ is

1. Generate a random sample $X_1, X_2, \cdots, X_m$ from $f(x)$.
2. For each $X_i$, compute $g(x) = I(X_i \leq x) = \begin{cases} 1, & X_i \leq x \\ 0, & X_i > x\end{cases}$.
3. Compute $\hat{F}(x) = \bar{g(X)} = \frac{1}{m} \sum_{i = 1}^m I(X_i \leq x)$ as an estimate of $F(x)$.

Note: $\hat{F}(x)$ is called the *empirical distribution* of $F(x)$.

Why does this work?

$E(x) = 1 \cdot P(X \leq x) + 0 \cdot p(X > x) = P(X \leq x) = F(x)$.

```{r}
# the "Hit-or-Miss" method:

# step 1.
x <- rnorm(10000)

# step 2. (an indicator function)
i <- x <= 1.96

# step 3.
mean(i)

norm_cdf <- function(x, n = 10000, mu = 0, sigma = 1) {
  X <- rnorm(n, mu, sigma)
  i <- X <= x
  mean(i)
}

exp_cdf <- function(x, n = 10000, lambda = 5) {
  X <- rexp(n, lambda)
  i <- X <= x
  mean(i)
}

exp_cdf(7.6, n = 1000, lambda = 5)
pexp(7.6, 5)

beta_cdf <- function(x, n = 10000, a = 2, b = 3) {
  X <- rbeta(n, a, b)
  i <- X <= x
  mean(i)
}

beta_cdf(0.7, a = 2, b = 3)
pbeta(0.7, 2, 3)
```

How do we choose between competing estimates for parameters $\hat{\theta}$?

Say we have $\hat{\theta_1}$ and $\hat{\theta_2}$ as estimators (?) of $\theta$. We care about:

1. *Unbiasedness:* if $E(\hat{\theta}) = \theta$, then $\hat{\theta}$ is an unbiased estimate.
2. *Minimum Variance:* if $Var(\hat{\theta_1}) < Var(\hat{\theta_2})$, we would prefer $\hat{\theta_1}$, other properties being equivalent.

Developed estimators should satisfy these properties.

"Uniformly Most powerful Variance among Unbiased Estimates": "UMVUE". 

Papers can be published based on whether a new estimator has less variance than an existing estimator.

How do we compare the variance of an estimator? Samples of samples.

```{r}
# compare hit or miss vs. simple monte-carlo
# theta_1 is simple monte-carlo
# theta_2 is indicator
# estimating F_Z(1.96) where Z ~ N(0, 1).
# generate 100 estimates of theta_1 and theta_2 (n = 10,000)
# then, take the variance of each

# for theta_1
z_cdf <- function(x, n = 1000) {
  u <- runif(n, 0, 1)

  # density of Z ~ N(0, 1)?
  theta <- function(x, u) {
    mean(1 / sqrt(2 * pi) * x * exp((-x^2 * u^2) / 2))
  }

  # Z ~ N(0, 1) is symmetric about 0
  if (x >= 0) {
    0.5 + theta(x, u)
  } else {
    1 - 0.5 - theta(-x, u)
  }
}

# for theta_2
z_cdf2 <- function(x, n = 1000, mu = 0, sigma = 1) {
  X <- rnorm(n, mu, sigma)
  i <- X <= x
  mean(i)
}

# wrap the 2 functions, and collect results
comparison <- function(x = 1.96, n = 100) {
  t1 <- numeric(n)
  t2 <- numeric(n)
  for (i in 1:n) {
    t1[i] <- z_cdf(x)
    t2[i] <- z_cdf2(x)
  }
  c(theta1 = var(t1), theta2 = var(t2))
}

comparison()
```

- According to Dr. Ko, three flavors of journals in statistics:
  - theoretical journals
  - simulation-based journals
  - applied journals?

# 3/7/2023

## Summary

Variance reduction methods, the Antithetic Variable Method

## Notes

Suppose our goal is to estimate a parameter $\theta$. Could be population mean, median, variance, etc.

*Point Estimation:* use sample data to calculate a single estimate $\hat{\theta}$ of the population value.

If you have competing estimators, we would choose the one that satisfies the following two criteria:

1. $E(\hat{\theta}) = \theta$; unbiasedness
2. Minimum variance 

This is called a Uniformly Minimum Variance Estimate among Unbiased Estimates: UMVUE.

We can apply Monte-Carlo integration to competing estimators to determine which among them have the lowest variance. We demonstrated this in the last class with the Normal CDF.

### Antithetic Variable Method

Suppose we have two random variables $X$ and $Y$. Then, $Var(aX \pm bY) = a^2 \cdot Var(X) + b^2 \cdot Var(Y) \pm 2 \cdot a \cdot b \cdot Cov(X, Y)$ where $a$ and $b$ are numbers, and $Cov(X, Y) = E(XY) - E(X) \cdot E(Y)$.

Ex. $Var(2X + 0.5Y)$

We have shown that 
$$
\frac{1}{m}\sum_{i=1}^n g(u_i) \to \theta = \int_a^b g(x)\ dx
$$

by the law of large numbers.

Now, let $a = b = \frac{1}{2}$, and thus
$$
Var(\frac{X + Y}{2}) = Var(\frac{1}{2}X + \frac{1}{2}Y) = (\frac{1}{2})^2Var(X) + (\frac{1}{2})^2Var(Y) + 2 \cdot \frac{1}{2} \cdot \frac{1}{2} \cdot Cov(X, Y)
$$

Then, let $u_1, u_2 \sim U(0, 1)$ and consider a function $h(\cdot).$

(a) if $u_1 \perp u_2$, then $h(u_1) \perp h(u_2)$ and 
$$
Var\Bigl(\frac{h(u_1) + h(u_2)}{2} \Bigr) = \frac{1}{4}(Var(h(u_1)) + Var(h(u_2)))
$$

```{r}
u <- runif(1000)
v <- 1 - u

# Cov(u, v) < 0!
cov(u, v)
```

Comparison between simple monte-carlo and antithetic variable approach.

```{r}
norm_cdf_anti <- function(x, n) {
  u <- runif(n / 2)
  v <- 1 - u
  y <- c(u, v)
  
  pos_cdf <- (1 / sqrt(2 * pi)) * abs(x) * exp(-x^2 * y^2 / 2) + 0.5
  
  if (x >= 0) {
    cdf <- pos_cdf
  } else {
    cdf <- 1 - pos_cdf
  }
  
  return(mean(cdf))
}

norm_cdf <- function(x, n) {
  u <- runif(n)
  
  pos_cdf <- (1 / sqrt(2 * pi)) * abs(x) * exp(-x^2 * u^2 / 2) + 0.5
  
  if (x >= 0) {
    cdf <- pos_cdf
  } else {
    cdf <- 1 - pos_cdf
  }
  
  return(mean(cdf))
}

s <- numeric(100)
a <- numeric(100)

for (i in 1:100) {
  s[i] <- norm_cdf(1.96, 100)
  a[i] <- norm_cdf_anti(1.96, 100)
}

var(s)
var(a)
```

### Control Variate Method

Let $\theta = E(g(X))$. An estimate of $\theta$ by control variate method $\hat{\theta}_c$ can be obtained by 

(a) finding a function $l(x)$ of a random variable such that (i) $\mu = E(l(x))$ is known and (ii) $l(x)$ is correlated with $g(x)$ and

(b) setting $\hat{\theta}_c = g(x) + c[l(x) - \mu]$.

Note that the random variable $l(x)$ is called a *control variate* and does not need to be a pdf. It is just a function of $x$.

So, how do we find $c$?

$$
\min_c Var(\hat{\theta}_c) = \min_c Var(1 \cdot g(x) + c \cdot [l(x) - \mu])
$$


# 3/9/2023

## Summary

More on the control variate method.

## Notes

- Homework 3 is due on 3/16 (next Thursday)

**Question from last class:** why are we able to use the SLLN with the antithetic variable method? Aren't we violating the assumption of i.i.d. draws used for estimation?

Standard Monte Carlo:

1. $u_1, u_2, u_3, u_4 \sim U(0, 1)$

2. $\hat{\theta}_4 = \frac{1}{4} \sum_{i = 1}^4 g(u_i)$

Antithetic Variable Method:

1. $u_1, u_2 \sim U(0, 1); u_3 = 1 - u_1, u_4 = 1 - u_2.$
    - Note that $Cov(u_1, u_3) < 0$ and $Cov(u_2, u_4) < 0.$

2. We then  estimate a new sample mean

\begin{align*}
  \theta \leftarrow \frac{1}{4} \sum_{i = 1}^4 g(u_i) &= \frac{g(u_1) + g(u_2) + g(u_3) + g(u_4)}{4} \\
  &= \frac{g(u_1) + g(1 - u_1) + g(u_2) + g(1 - u_2)}{4} \\ \\
  &= \frac{\frac{g(u_1) + g(1 - u_2)}{2} + \frac{g(u_2) + g(1 - u_2)}{2}}{2} \\
  &= \frac{1}{2} \sum_{i = 1}^2 g^*(u_i).
\end{align*}

So, you end up with half the number of realizations, but $\frac{g(u_1) + g(1 - u_2)}{2}$ and $\frac{g(u_2) + g(1 - u_2)}{2}$ are independent from each other. So, for estimation, they constitute independent and identical draws, and thus we can rely on the SLLN to know that our estimate of $\theta$ converges to the true value as the number of realizations used increases.

### Control Variate Method

Picking up where we left off.

$$
\min_c Var(\hat{\theta}_c) = \min_c Var(1 \cdot g(x) + c \cdot [l(x) - \mu])
$$

We assume $\mu$, the mean of $l(x)$ is known. In practice, we can estimate it empirically from simulated data?

::: {.column-margin}
Note that $Var(X + a) = Var(X)$
:::
\begin{align*}
  \min_c Var(\hat{\theta}_c) &= \min_c Var(1 \cdot g(x) + c \cdot [l(x) - \mu]) \\
  &\implies \min_c 1 \cdot Var(g(x)) + c^2 \cdot Var[l(x) - \mu] + 2 \cdot 1 \cdot c \cdot Cov(g(x), l(x)) \\ 
  &\implies \min_c Var(l(x) - \mu) \cdot c^2  + 2 \cdot Cov(g(x), l(x)) \cdot c + Var(g(x)) \\
  &\implies \frac{d}{dc} \Biggl[ Var(l(x) - \mu) \cdot c^2  + 2 \cdot Cov(g(x), l(x)) \cdot c + Var(g(x)) \Biggr] \equiv 0 \\
  &\implies 2 \cdot Var(l(x) - \mu) \cdot c^* + 2 \cdot Cov(g(x), l(x)) \equiv 0 \\
  &\implies c^* = -\frac{Cov(g(x), l(x))}{Var(l(x))}
\end{align*}

Is $c^*$ the minimum? Yes:
$$
 \frac{d^2}{dc^2} Var(\hat{\theta}_c) \Big|_{c = c^*} > 0.
 $$

Variance is always positive by definition, so we know that the value found for $c^*$ is the minimum.

$$
c^* = -\frac{Cov(g(x), l(x))}{Var(l(x))}
$$

**Example:**

$$
\theta = E(e^u) = \int_0^1 e^u\ du
$$

So $g(u) = e^u.$ First, we need a candidate for $l(x)$.

How about $l(u) = u$ where $u \sim U(0, 1).$ So, $E(U) = 0.5$ and $Var(U) = \frac{1}{12}$.

::: {.column-margin}
We solved for these values in Homework 1.
:::

Then
$$
\hat{\theta}_c = e^u + c \cdot [u - \frac{1}{2}]
$$
where
$$
  c = -\frac{Cov(e^u, u)}{Var(u)} = -0.1409 / (1/12).
$$

```{r}
u <- runif(100)
C <- -cov(exp(u), u) / var(u)
C

-0.1409 / (1/12)
```

**Trying on our own:**

Estimate $\theta = \int_0^1 e^{-x} / (1 + x^2)\ dx$ using the control variate method with $l(x) = e^{-0.5} / (1 + x^2)$ and an emperical value of $c^*$.


```{r}
g <- function(x) exp(-x) / (1 + x^2)
l <- function(x) exp(-0.5) / (1 + x^2)

# estimating C
u <- runif(10000)
C <- -cov(g(u), l(u)) / var(l(u))

# simple monte-carlo vs. control variate
u <- runif(1000)
smc <- mean(g(u))
hat <- mean(g(u) + C * (l(u) - mean(l(u))))

integrate(g, 0, 1)
print(smc)
print(hat)
print(var(g(u)))
print(var(g(u) + C * (l(u) - mean(l(u)))))
```

# 3/14/23

## Summary

Importance sampling. Should refer to Dr. Ko's notes and finish this section.

## Notes

See example 6.7.

Let $x_1, x_2, \cdots, x_n \sim (\mu, \sigma^2)$

$$
Var(\bar{X}) = Var(\frac{1}{n} \sum_{i = 1}^n x_i) = Var(\frac{1}{n}x_1 + \frac{1}{n}x_2 + \cdots + \frac{1}{n}x_n) =
$$

Better to use $Var(g(x_i)) / n$? rather than $Var(\hat{\theta}_1, \hat{\theta}_2, \cdots, \hat{\theta}_m).$

```{r}
n <- 10000
u <- runif(n)
# u1, u2, ..., u10k --> e^u1, e^u2, ... e^u10k

hat_theta <- mean(exp(u))
est_var <- var(exp(u))

hat_theta <- numeric(100)
for (i in 1:100) {
  u <- runif(10000)
  hat_theta[i] <- mean(exp(u))
}

var(hat_theta)
```

::: {.column-margin}
Dr. Ko: once you've taken MATH-462, you should be able to study new topics in statistics on your own.
:::

### Importance Sampling

$$
\theta = \int_0^1 g(x)\ dx
$$

**Simple Monte-Carlo**

1. Generate $u_1, u_2, ..., u_n \sim U(0, 1)$
2. $\hat{\theta} = \frac{1}{n} \sum_{i=1}^n g(u_i)$

**Importance sampling**

1. Generate $x_1, x_2, ..., x_n \sim f_X(x)$
2. $\hat{\theta} \leftarrow \frac{1}{n} \sum_{i = 1}^n (g(xi) / f(xi))$

Here, $f(x)$ is known as an *importance* function; it is a PDF. We need it to be easy to generate numbers from the importance function. Note a key assumption: $f(x) > 0$ wherever $g(x) > 0$. That is, support of $f(x)$ is not necessarily the same as that of $g(x)$.

Let $\theta = \int g(x)dx$ and $X$ be a random variable with pdf $f(x)$ such that $f(x) > 0$ on the set $\{x: g(x) > 0 \}$.


...


How to pick $f(X)$?

- $Var(\hat{\theta}_I) = \

```{r}
# Example 6.11
x <- seq(0, 1, 0.01)
g <- exp(-x) / (1 + x^2)
f3 <- exp(-x) / (1 - exp(-1))

plot(x, g, type = "l", ylim = c(0, 2))
lines(x, f3, lty = 3)
```

# 3/17/23

## Summary

Ch.7, Monte Carlo methods in Statistical Inference

## Notes

- We're going to cover the topic that has motivated our learning of the preceding topics.
- Monte-Carlo methods involve use/generation of random numbers.
  - first we learned how to *generate* random numbers
  - then we learned how to use random numbers to perform *integration*
      - $\theta = E[g(x)]$
      - $E(X) = \int_{-\infty}^{\infty} x * f_X(x)\ dx \to \hat{\theta} = \frac{1}{n} \sum_{i = 1}^n x_i \to \theta$

### Mean Square Error (MSE)

Suppose $\hat{\theta}$ is an estimate of $\theta$. Define **Mean Square Error** of $\hat{\theta}$ as 
\begin{align*}
  MSE[\hat{\theta}] &= E[(\hat{\theta} - \theta)^2] \\
    &= E[\hat{\theta}^2 - 2 \cdot \theta \cdot \hat{\theta} + \theta^2] \\
    &= E[\hat{\theta}^2] - 2 \theta E[\hat{\theta}] + \theta^2 \\
    &= E[\hat{\theta}^2] - E[\hat{\theta}]^2 + E[\hat{\theta}]^2 - 2 \theta E[\hat{\theta}] + \theta^2 \\
    &= Var(\hat{\theta}) + (E[\hat{\theta}] - \theta)^2
\end{align*}

::: {.column-margin}
Note that $\hat{\theta}$ is an estimate, and $\theta$ is the *true* value of a parameter.
:::

::: {.column-margin}
MSE can be decomposed into two components. The second term, $(E[\hat{\theta}] - \theta)^2$, can be thought of as a penalty for the *bias* of $\hat{\theta}$.
:::

Some examples.

```{r}
# Example

m <- 100
Y <- numeric(m)

for (i in 1:m) {
  x <- rnorm(3, 0, 1)
  Y[i] <- sum(x^2)
}

(hat_theta <- mean(Y))  # should be close to 3
(hat_mse <- mean((Y - 3)^2))  # should be close to 6
```

```{r}
# Example 7.1
m <- 100
L <- numeric(m)

for (i in 1:m) {
  x <- rnorm(2, 0, 1)
  L[i] <- abs(x[1] - x[2])
}

(hat_theta <- mean(L))  # should be close to 2 / sqrt(pi)
(hat_mse <- mean((L - 2 / sqrt(pi))^2))
```

Kth-level trimmed mean.
$$
\bar{X}_{[-k]} = \frac{1}{n - 2 \cdot k} = \sum_{i = k + 1}^{n - k} x_{(i)}
$$

- Removes the top/bottom $k$ values from a sample and estimates the mean.
    - Used to eliminate extreme values.

```{r}
# Example 7.2

m <- 100
k <- 3
K <- numeric(m)

for (i in 1:m) {
  x <- rnorm(100)
  x <- sort(x)
  K[i] <- mean(x[k:(length(x) - k)])
}

(hat_theta <- mean(K))
```

Next topics: hypothesis testing, confidence intervals.

# 3/28/23

## Summary

Simulating confidence intervals for a mean.

## Notes

- Exam 2 this coming Thursday (3/30)
  - topics in chapter 6 (those covered in class)
  - 1 day take-home exam, due 1:30pm 3/31
  - submitted via email (pdf file) to kyungdukko at boisestate.edu
  - 4 to 6 big questions
  - Ch. 6 Simple Monte Carlo Integration
    - Variance Reduction Techniques
      - Antithetic
      - Control Variate
      - Importance Sampling

Shifting to discuss topics from Chapter 7: Monte-Carlo Methods for Statistical Inference. Last class, we compared mean-square errors for different methods. Now we'll talk about inference, "something you'll use the most in your statistical research".

Inference consists of two areas:

- Estimation
  - Estimation of unknown parameters $\theta$
    - *point* estimation: calculate **one number** from a random sample of size $n$, $x_1, x_2, \dots, x_n$.
      - typical approach: maximum likelihood estimation
    - *interval* estimation: $100 \cdot (1 - \alpha)$% confidence interval of $\theta$.
      - Calculate $(L, U)$, the bounds, for unknown $\theta$.
      - Expect $\theta$ to be included in the interval $(L, U)$ with confidence of $100 \cdot (1 - \alpha)$%.
  - We've already covered point estimation.

- Test of Hypothesis

Today, we'll focus on interval estimation. Elementary statistics give us some formulas for interval estimation. Estimating intervals for and unknown population mean $\mu$ and population variance $\sigma^2$.

Procedure:

1. We need a sample of size $n$, $x_1, x_2, \dots, x_n$

2. We specify our confidence level, choosing a value for $\alpha$ in $100 \cdot (1 - \alpha)$.
    - Dr. Ko says it's most common for 90%, 95%, or 99% CIs to be generated.

3. One of the following conditions:
    - (I) $x_1, x_2, \dots, x_n \sim N(\mu, \sigma^2)$
        - $\mu$ is unknown, $\sigma^2$ is known, $n \geq 2$
    - (II) $x_1, x_2, \dots, x_n \sim N(\mu, \sigma^2)$
        - $\mu$ is unknown, $\sigma^2$ is unknown, $n < 30$
    - (III) $x_1, x_2, \dots, x_n \sim (\mu, \sigma^2)$
        - $\mu$ is unknown, $\sigma^2$ is unknown, $n > 30$

Formula for I: $(\bar{X} - Z_{\frac{\alpha}{2}} \cdot \frac{\sigma}{\sqrt{n}}, \bar{X} + Z_{\frac{\alpha}{2}} \cdot \frac{\sigma}{\sqrt{n}})$, where $\bar{X} = \sum_{i = 1}^n x_i / n$.

Formula for II: $(\bar{X} - T_{\frac{\alpha}{2}, df = n-1} \cdot \frac{s}{\sqrt{n}}, \bar{X} + T_{\frac{\alpha}{2}, df = n-1} \cdot \frac{s}{\sqrt{n}})$, where $\bar{X} = \sum_{i = 1}^n x_i / n$.

Formula for III: $(\bar{X} - Z_{\frac{\alpha}{2}} \cdot \frac{s}{\sqrt{n}}, \bar{X} + Z_{\frac{\alpha}{2}} \cdot \frac{s}{\sqrt{n}})$, where $\bar{X} = \sum_{i = 1}^n x_i / n$.
  - By the Central Limit Theorem, $\bar{X} \overset{approx}{\sim} N(\mu, \frac{\sigma}{\sqrt{n}}^2)$

Examples.

```{r}
x <- c(580, 400, 428, 825, 850, 875, 920, 550, 575, 750, 636, 360, 590, 735, 950)
n <- length(x)
m <- mean(x)
s <- sd(x)
a <- c(0.10, 0.05)
t <- qt(1 - a / 2, df = n - 1)

(c90 <- c(m - t[1] * s / sqrt(n), m + t[1] * s / sqrt(n)))
(c95 <- c(m - t[2] * s / sqrt(n), m + t[2] * s / sqrt(n)))
```

```{r}
n <- 4
m <- 65
s <- 2.4
t <- qt(1 - 0.05/2, df = 3)

(c95 <- c(m - t * s / sqrt(n), m + t * s / sqrt(n)))
```

How do we know if a method for estimating a CI is valid? We have to prove that that the estimator meets the property ($\theta$ contained within the interval, at probability $1 - \alpha$).

Empirical confidence level, for the t-statistic interval estimation. Let $\mu$ and $\sigma$ be fixed. Draw a random sample of size $n$ from the normal distribution with paramters $\mu$ and $\sigma$, and compute a 95% CI interval from the sample. Repeat this $k$ times, and then compute the proportion of estimated intervals that contain $\mu$.

\begin{align*}
  (L, U)_1 &\leftarrow x^1_1, x^1_2, \dots, x^1_n \sim N(\mu, \sigma) \\
  (L, U)_2 &\leftarrow x^2_1, x^2_2, \dots, x^2_n \sim N(\mu, \sigma) \\
  &\vdots \\
  (L, U)_k &\leftarrow x^k_1, x^k_2, \dots, x^k_n \sim N(\mu, \sigma)
\end{align*}

In R, this can be accomplished, using $k = 1000$, $n = 20$, $\mu = 15$, $\sigma = 3$, with an alpha of 0.05.

```{r}
t_ci <- function(n = 20, mu = 15, sigma = 3, alpha = 0.05) {
  x <- rnorm(n, mu, sigma)
  m <- mean(x)
  s <- sd(x)
  t <- qt(1 - alpha / 2, df = n - 1)

  m + (t * s / sqrt(n)) * c(-1, 1)
}

res <- replicate(n = 1000, expr = t_ci(), simplify = FALSE)
(chk <- res |> map_lgl(~between(15, .[1], .[2])) |> mean())

res |>
  map_df(enframe, .id = "m") |>
  mutate(m = as.numeric(m)) |>
  pivot_wider(names_from = name, values_from = value) |>
  set_names(c("m", "l", "u")) |>
  group_by(m) |>
  mutate(contains_truth = between(15, l, u)) |>
  ggplot(aes(x = m, ymin = l, ymax = u, color = contains_truth)) +
  geom_linerange() +
  geom_hline(yintercept = 15, lty = "dashed") +
  scale_color_manual(values = c("darkblue", "skyblue")) +
  ylim(8, 22) +
  theme(legend.position = "top")
```

# 4/4/23

## Summary

Confidence interval for variance, hypothesis testing.

## Notes

We'll get back our exam results via email on Thursday. 

**Today:** simulations of confidence intervals. Suppose we have a random sample
$$
X_1, X_2, \cdots, X_n \sim N(\mu, \sigma^2)
$$

where: $\mu$ and $\sigma^2$ are unknown, and $n < 30$.

Our formula, based on this scenario, for a $100 \cdot (1 - \alpha)%$ CI of $\mu$:
$$
(\bar{X} - t_{\frac{\alpha}{2}, n - 1} \cdot \frac{s}{\sqrt{n}}, \bar{X} + t_{\frac{\alpha}{2}, n - 1} \cdot \frac{s}{\sqrt{n}})
$$

Last time, we checked to see (empirically) if the interval formula worked as intended.

Now, again with a random sample from a normal distribution, let's check a one-sided confidence interval for unknown $\sigma^2$, with proposed formula
$$
\Big(0, \frac{(n - 1) \cdot s^2}{\chi^2_{\alpha, n - 1}} \Big), s^2 = \frac{\sum_{i = 1}^n (x_i - \bar{x})^2}{n - 1}.
$$

Check if this is valid using $\mu = 0, \sigma^2 = 4, n = 20, m = 10,000$.

```{r}
s_ci <- function(mu = 0, sigma = 2, n = 20, alpha = 0.05) {
  x <- rnorm(n = n, mean = mu, sd = sigma)
  v <- var(x)
  chi <- qchisq(alpha, n - 1)

  c(0, (n - 1) * v / chi)
}

res <- replicate(10000, s_ci(), simplify = FALSE)
(chk <- res |> map_lgl(~between(4, .[1], .[2])) |> mean())
```

### Testing of Hypotheses

Given: a claim about a parameter $\theta$:

1. Choose a significance level ($\alpha$), typically 0.01, 0.05, or 0.10.

2. Set up statistical hypotheses based on the given claim
    - $H_1$ or $H_a$, the *alternative* hypothesis
    - $H_0$, the *null* hypothesis
    - Note: $H_0 \cap H_1 = \{\}$ (they must be mutually exclusive)
    - Conventions: the claim is typically set as $H_1$, ***but*** equality should be on $H_0$.

**Example 1.** The average annual income of workers who majored in mathematics is greater than 100,000. Let $\mu$ = the average annual income. Then, $H_1: \mu > 100000$, and $H_0: \mu \leq 100000$.

**Example 2.** The average amount of rubbing alcohol in containers is at least 12 oz. Let $\mu = 12$. Then, $H_1: \mu < 12$ and $H_0: \mu \geq 12$.

**Example 3.** The average IQ of mathematics students is 130. Let $\mu = 130$. Then $H_1: \mu \neq 130$ and $H_0: \mu = 130$.

3. Calculate the Test Statistic (T.S.) under the assumption that $H_0$ is true.

The T.S. value is a function of the sample $x_1, x_2, \cdots, x_n$ and other known information for the population.

4. Find the rejection region of $H_0$ based on the probability distribution of the Test Statistic.

5. Decision: if T.S. is in the rejection region, then we reject the null hypothesis $H_0$.

Steps 3 and 4 are the theoretical aspects of hypothesis testing. We cover these two in Math 462 / Math 562.

| Nature        | Accept $H_0$  | Accept $H_1$ |
| :------------ | :------------ | :----------- |
| $H_0$ is true | Correct       | Type I error | 
| $H_1$ is true | Type II error | Correct      |

Type I error = $P[\text{reject } H_0 | H_0 \text{ is true}] = \alpha$

Type II error = $P[\text{reject } H_1 | H_1 \text{ is true}] = \beta$

Note: $1 - \beta$ is called the *power* of a test.

Goal: to minimize both types of error. *But*, it's not possible to minimize both at the same time. The hypotheses are mutually exclusive. If we minimize one type, the probability of the other will increase.

So, statistical hypothesis testing takes the following form:

1. fix the type I error to a small number (0.01, 0.05, 0.10)
2. minimize type II error.

A test that accomplishes these things is called the Uniformly Most Powerful Test (UMP Test).

# 4/6/23

## Summary

Computation of test statistics.

## Notes

Review of question 5 from the exam.

We want $\theta = \int_0^1 h(x)\ dx$, where $h(x) = -0.5x + 0.5$, using the hit-or-miss method. For this procedure, we generate two uniform variables from $U(0, 1)$, $u_1$ and $u_2$.

TODO: typeset this
```
0 <= h(x) <= b
x = u1
y = b * u2

0 <= h(x) <= -0.5x + 0.5 <= 0.5
x = 0 -> h(0) = 0.5 = b
x = 1 -> h(1) = 0
```

Final exam will consist of new topics after Exam 2, but also topics from before. It's cumulative. One question from each exam will be included in the final.

Continuing with hypothesis testing.

5. Decision making: if T.S. is in the rejection region, then we reject $H_0$.

So, how do we determine the rejection region?

- tests of equality $H_0: \theta = \theta_0$ vs. $H_1: \theta \neq \theta_0$ are *two-sided tests* (area of each region is $\alpha / 2$).
- testing claims that a quantity is greater than some value result in rejection regions on the right-tail
- testing claims that a quantity is larger than some value result in rejection regions on the left-tail

**Example.** Test of hypothesis for an unknown population mean $\mu$. Let $X_1, X_2, \cdots, X_n$ be a random sample.

Scenario I. $\mu$ is unknown, $n \geq 2$ and $\sigma^2$ is known. $X_1, X_2, \cdots, X_n \sim N(\mu, \sigma^2)$.
    - Assuming $H_0$ is true: $T.S.^I = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} \sim N(0, 1)$

Scenario II. $\mu$ is unknown, $n < 30$ and $\sigma^2$ is unknown. $X_1, X_2, \cdots, X_n \sim N(\mu, \sigma^2)$.
    - Assuming $H_0$ is true: $T.S.^{II} = \frac{\bar{x} - \mu_0}{s / \sqrt{n}} \sim t(df = n - 1)$

Scenario III. $\mu$ is unknown, $n > 30$ and $\sigma^2$ is unknown. $X_1, X_2, \cdots, X_n \sim (\mu, \sigma^2)$.
    - Assuming $H_0$ is true: $T.S.^{III} = \frac{\bar{x} - \mu_0}{s / \sqrt{n}} \overset{approx}{\sim} N(0, 1)$


**Example.** The heights of young American men, in inches, are normally distributed with unknown mean $\mu$ and unknown standard deviation $\sigma$. You select a random sample of 4 young American men and measure their heights. The sample mean of the 4 measurements is 65 inches and the sample standard deviation is 2.4 inches.

Someone claims that the mean height of young American men is 70 inches. Construct $H_0$ and $H_1$.

$H_0: \mu = 70$, $H_1: \mu \neq 70$. This is scenario II.

$T.S.^{II} = \frac{65 - 70}{-2.4 / \sqrt{4}} \approx 4.16$.

```{r}
65 - qt(0.025, 3) * 2.4 / 2
65 + qt(1 - 0.025, 3) * 2.4 / 2
```

# 4/11/23

## Summary

## Notes

